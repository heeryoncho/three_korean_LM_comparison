{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzJHRniu_wnb"
   },
   "source": [
    "### KR-BERT Tokenizer로 특징 단어를 추출하여 나이브 베이즈와 로지스틱 회귀 분류기를 구축함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 684,
     "status": "ok",
     "timestamp": 1620897167189,
     "user": {
      "displayName": "Heeryon Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhbUw0cGXedemjEJCfZgrHbhCHEnnGel5FvhM9W=s64",
      "userId": "17497106563464165932"
     },
     "user_tz": -540
    },
    "id": "pHn2WjI2AyfY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"snunlp/KR-BERT-char16424\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example, padding=False, truncation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1089,
     "status": "ok",
     "timestamp": 1620899392485,
     "user": {
      "displayName": "Heeryon Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhbUw0cGXedemjEJCfZgrHbhCHEnnGel5FvhM9W=s64",
      "userId": "17497106563464165932"
     },
     "user_tz": -540
    },
    "id": "7InhIC2QA7hi"
   },
   "outputs": [],
   "source": [
    "def vectorize(data_train, data_val, data_test):\n",
    "    train_doc = data_train[\"document\"].str.replace(\"[[문단]] \",\"\", regex=True)\n",
    "    val_doc = data_val[\"document\"].str.replace(\"[[문단]] \",\"\", regex=True)\n",
    "    test_doc = data_test[\"document\"].str.replace(\"[[문단]] \",\"\", regex=True)\n",
    "    \n",
    "    encodings_train = train_doc.map(tokenize_function)\n",
    "    encodings_val = val_doc.map(tokenize_function)\n",
    "    encodings_test = test_doc.map(tokenize_function)\n",
    "\n",
    "    result_train = [' '.join(str(x) for x in each['input_ids']) for each in encodings_train]\n",
    "    result_val = [' '.join(str(x) for x in each['input_ids']) for each in encodings_val]\n",
    "    result_test = [' '.join(str(x) for x in each['input_ids']) for each in encodings_test]\n",
    "    #print(result_val)\n",
    "    \n",
    "    vect = CountVectorizer(lowercase=False)\n",
    "    X_train = vect.fit_transform(result_train)\n",
    "    X_val = vect.transform(result_val)\n",
    "    X_test = vect.transform(result_test)\n",
    "\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHWc_2ohJRm9"
   },
   "source": [
    "### 실험 데이터는 아래의 URL에서 다운로드 받을 수 있음\n",
    "\n",
    "http://aihumanities.org/ko/archive/data/?vid=1\n",
    "\n",
    "바로 밑의 셀을 실행하여 다운로드한 데이터의 압축을 풀고, 폴더명을 'data'로 변경함.\n",
    "\n",
    "이 때 폴더 구조와 파일의 위치(예시)는 다음과 같음.\n",
    "\n",
    "`data/job/train_0.txt `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! unzip ./korean_essay_score_range_prediction_dataset.zip -d ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MN1EmnprCFDD"
   },
   "outputs": [],
   "source": [
    "# 나이브 베이즈 또는 로지스틱 회귀 실험 결과를 취득\n",
    "# 아래 clf 에서 분류기 하나를 선택\n",
    "\n",
    "#clf = MultinomialNB()\n",
    "clf = LogisticRegression(random_state=0, max_iter=5000)\n",
    "\n",
    "experiments = [\"job\", \"job_econ\", \"job_succ\", \"job_happ\", \"job_econ_succ_happ\", \n",
    "               \"happiness\", \"happiness_econ\", \"happiness_succ\", \"happiness_job\", \"happiness_econ_succ_job\", \n",
    "               \"all\"]\n",
    "\n",
    "for exp in experiments:\n",
    "    print(\"======================\")\n",
    "    print(\"result_{}\".format(exp))\n",
    "    print(\"======================\")\n",
    "    avg_acc_train = []\n",
    "    avg_acc_val = []\n",
    "    avg_acc_test = []    \n",
    "    for i in range(7):\n",
    "        folder = ''\n",
    "        if exp.startswith(\"job\"):\n",
    "            folder = 'job'\n",
    "        elif exp.startswith(\"happiness\"):\n",
    "            folder = 'happiness'\n",
    "        else:\n",
    "            folder = 'all'\n",
    "        \n",
    "        data_train = pd.read_csv(\"data/{}/train_{}.txt\".format(folder, i), sep='\\t')\n",
    "        data_val = pd.read_csv(\"data/{}/val_{}.txt\".format(folder, i), sep='\\t')\n",
    "        data_test = pd.read_csv(\"data/{}/test_{}.txt\".format(folder, i), sep='\\t')\n",
    "        \n",
    "        if 'econ' in exp:\n",
    "            data_train_econ = pd.read_csv(\"data/economic.txt\", sep='\\t')\n",
    "            data_train = pd.concat([data_train, data_train_econ], ignore_index=True)\n",
    "\n",
    "        if 'succ' in exp:\n",
    "            data_train_succ = pd.read_csv(\"data/success.txt\", sep='\\t')\n",
    "            data_train = pd.concat([data_train, data_train_succ], ignore_index=True)\n",
    "            \n",
    "        if (folder == 'job') and 'happ' in exp:            \n",
    "            data_train_happ = pd.read_csv(\"data/happiness.txt\", sep='\\t')\n",
    "            data_train = pd.concat([data_train, data_train_happ], ignore_index=True)\n",
    "\n",
    "        if (folder == 'happiness') and 'job' in exp:            \n",
    "            data_train_job = pd.read_csv(\"data/job.txt\", sep='\\t')\n",
    "            data_train = pd.concat([data_train, data_train_job], ignore_index=True)            \n",
    "\n",
    "        train_label = data_train[\"label\"]\n",
    "        val_label = data_val[\"label\"]\n",
    "        test_label = data_test[\"label\"]\n",
    "        \n",
    "        X_train, X_val, X_test = vectorize(data_train, data_val, data_test)\n",
    "        \n",
    "        print(\"X_train\", X_train.shape)\n",
    "\n",
    "        clf.fit(X_train, train_label)\n",
    "        pred_train = clf.predict(X_train)\n",
    "        pred_val = clf.predict(X_val)\n",
    "        pred_test = clf.predict(X_test)\n",
    "\n",
    "        '''\n",
    "        print(\"X_test\", X_test.shape)\n",
    "        print(\"y_test\", len(test_label))\n",
    "        print(\"X_val\", X_val.shape)\n",
    "        print(\"y_val\", len(val_label))\n",
    "        print(\"X_train\", X_train.shape)\n",
    "        print(\"y_train\", len(train_label))\n",
    "        '''\n",
    "\n",
    "        acc_train = accuracy_score(pred_train, train_label)\n",
    "        avg_acc_train.append(acc_train)\n",
    "\n",
    "        acc_val = accuracy_score(pred_val, val_label)\n",
    "        avg_acc_val.append(acc_val)\n",
    "\n",
    "        acc_test = accuracy_score(pred_test, test_label)\n",
    "        avg_acc_test.append(acc_test)\n",
    "\n",
    "        print(\"acc_train:\", round(acc_train, 5))\n",
    "        print(\"acc_val:\", round(acc_val, 5))\n",
    "        print(\"acc_test:\", round(acc_test, 5))\n",
    "        print(\"-------------------\")\n",
    "\n",
    "    avg_train = sum(avg_acc_train) / len(avg_acc_train)\n",
    "    avg_val = sum(avg_acc_val) / len(avg_acc_val)\n",
    "    avg_test = sum(avg_acc_test) / len(avg_acc_test)\n",
    "\n",
    "    print(\"AVG_TRAIN:\", round(avg_train, 5))\n",
    "    print(\"AVG_VAL:\", round(avg_val, 5))\n",
    "    print(\"AVG_TEST:\", round(avg_test, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LKaYuCWHXYv"
   },
   "source": [
    "### 로지스틱 회귀와 나이브 베이즈 모델 구축 시 사용된 특징 단어 확인 및 로지스틱 회귀에서의 각 클래스 별 특징 단어 상위 10위 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5831,
     "status": "ok",
     "timestamp": 1620899943818,
     "user": {
      "displayName": "Heeryon Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhbUw0cGXedemjEJCfZgrHbhCHEnnGel5FvhM9W=s64",
      "userId": "17497106563464165932"
     },
     "user_tz": -540
    },
    "id": "jPIAJealZqn4"
   },
   "outputs": [],
   "source": [
    "# https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\n",
    "\n",
    "train_data_file = \"data/all/train_6.txt\"\n",
    "data_train = pd.read_csv(train_data_file, sep='\\t')\n",
    "train_doc = data_train[\"document\"].str.replace(\"[[문단]] \",\"\", regex=True)\n",
    "train_label = data_train[\"label\"]\n",
    "\n",
    "parser = Komoran()\n",
    "\n",
    "temp_train = []\n",
    "for doc in train_doc:\n",
    "    temp_train.append(parser.morphs(doc))\n",
    "result_train = [' '.join(tokens) for tokens in temp_train]\n",
    "\n",
    "vect = CountVectorizer()\n",
    "X_train = vect.fit_transform(result_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1620899946541,
     "user": {
      "displayName": "Heeryon Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhbUw0cGXedemjEJCfZgrHbhCHEnnGel5FvhM9W=s64",
      "userId": "17497106563464165932"
     },
     "user_tz": -540
    },
    "id": "x_ChOt-cbIK9"
   },
   "outputs": [],
   "source": [
    "sum_words = X_train.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 992,
     "status": "ok",
     "timestamp": 1620899949287,
     "user": {
      "displayName": "Heeryon Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhbUw0cGXedemjEJCfZgrHbhCHEnnGel5FvhM9W=s64",
      "userId": "17497106563464165932"
     },
     "user_tz": -540
    },
    "id": "pzixYTUxbQiJ",
    "outputId": "0449a660-20ac-419a-b00a-1ebf744c889d"
   },
   "outputs": [],
   "source": [
    "sum_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 814,
     "status": "ok",
     "timestamp": 1620899954385,
     "user": {
      "displayName": "Heeryon Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhbUw0cGXedemjEJCfZgrHbhCHEnnGel5FvhM9W=s64",
      "userId": "17497106563464165932"
     },
     "user_tz": -540
    },
    "id": "ilgg4rICbUro"
   },
   "outputs": [],
   "source": [
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vect.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bCv5lh5c2m6"
   },
   "outputs": [],
   "source": [
    "words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1620899962004,
     "user": {
      "displayName": "Heeryon Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhbUw0cGXedemjEJCfZgrHbhCHEnnGel5FvhM9W=s64",
      "userId": "17497106563464165932"
     },
     "user_tz": -540
    },
    "id": "fHhInDieNyfd",
    "outputId": "bb7a9a03-58ab-488f-b633-71cd07472760",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(words_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 945,
     "status": "ok",
     "timestamp": 1620899964504,
     "user": {
      "displayName": "Heeryon Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhbUw0cGXedemjEJCfZgrHbhCHEnnGel5FvhM9W=s64",
      "userId": "17497106563464165932"
     },
     "user_tz": -540
    },
    "id": "DQws1BwvOR-l"
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, max_iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1215,
     "status": "ok",
     "timestamp": 1620899966827,
     "user": {
      "displayName": "Heeryon Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhbUw0cGXedemjEJCfZgrHbhCHEnnGel5FvhM9W=s64",
      "userId": "17497106563464165932"
     },
     "user_tz": -540
    },
    "id": "PKerlfC6OTEV",
    "outputId": "f9fcbeaf-da6e-4bde-e484-587d3309c3cb"
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1170,
     "status": "ok",
     "timestamp": 1620899969298,
     "user": {
      "displayName": "Heeryon Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhbUw0cGXedemjEJCfZgrHbhCHEnnGel5FvhM9W=s64",
      "userId": "17497106563464165932"
     },
     "user_tz": -540
    },
    "id": "0i6c7nXZQTX2",
    "outputId": "bab13104-c1c9-4aad-c779-1349b16ed646"
   },
   "outputs": [],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 838,
     "status": "ok",
     "timestamp": 1620900036103,
     "user": {
      "displayName": "Heeryon Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhbUw0cGXedemjEJCfZgrHbhCHEnnGel5FvhM9W=s64",
      "userId": "17497106563464165932"
     },
     "user_tz": -540
    },
    "id": "au7XvSbIOei1",
    "outputId": "4efac4ee-5464-4dda-e268-3171dfe801f0"
   },
   "outputs": [],
   "source": [
    "weight = clf.coef_\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 954,
     "status": "ok",
     "timestamp": 1620899975063,
     "user": {
      "displayName": "Heeryon Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhbUw0cGXedemjEJCfZgrHbhCHEnnGel5FvhM9W=s64",
      "userId": "17497106563464165932"
     },
     "user_tz": -540
    },
    "id": "fUDRrl8HQcNm"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1433,
     "status": "ok",
     "timestamp": 1620899980722,
     "user": {
      "displayName": "Heeryon Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhbUw0cGXedemjEJCfZgrHbhCHEnnGel5FvhM9W=s64",
      "userId": "17497106563464165932"
     },
     "user_tz": -540
    },
    "id": "q-o4YmQROj7d"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "vocab_idx = {y:x for x,y in vect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1030,
     "status": "ok",
     "timestamp": 1620899986696,
     "user": {
      "displayName": "Heeryon Cho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhbUw0cGXedemjEJCfZgrHbhCHEnnGel5FvhM9W=s64",
      "userId": "17497106563464165932"
     },
     "user_tz": -540
    },
    "id": "byqRBPfTQ7LW",
    "outputId": "e2c68662-ddae-4ea4-fac4-ead63d2b4f9d"
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(\"\\nLabel:\", str(i))\n",
    "    sel_weights = np.argsort(-weight[i])[:10]\n",
    "    for w in sel_weights:\n",
    "        print(vocab_idx[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning tokenizer\n",
    "encodings_train = train_doc.map(tokenize_function)\n",
    "result_train = [' '.join(str(x) for x in each['input_ids']) for each in encodings_train]\n",
    "\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "X_train = vect.fit_transform(result_train)\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter=1000)\n",
    "clf.fit(X_train, train_label)\n",
    "\n",
    "clf.classes_\n",
    "weight = clf.coef_\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_idx = {y:x for x,y in vect.vocabulary_.items()}\n",
    "for i in range(4):\n",
    "    print(\"\\nLabel:\", str(i))\n",
    "    sel_weights = np.argsort(-weight[i])[:10]\n",
    "    for w in sel_weights:\n",
    "        print(tokenizer.decode(int(vocab_idx[w])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPcPsS0gI/F18nvgB2yJwIQ",
   "collapsed_sections": [],
   "name": "korean_essay_score_range_prediction_NBLR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
